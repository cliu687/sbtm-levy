{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量来指定使用的GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 仅使用0号GPU\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "from jax import vmap\n",
    "import optax\n",
    "import numpy as onp\n",
    "from typing import Callable, Tuple\n",
    "import sys\n",
    "import argparse\n",
    "# 设置环境变量\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.75'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nicholas M. Boffi\n",
    "7/29/22\n",
    "\n",
    "Neural networks for score-based transport modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import haiku as hk\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from typing import Optional, Tuple, Callable, Union\n",
    "import jax\n",
    "from jax import vmap\n",
    "from dataclasses import dataclass\n",
    "\n",
    "###################################################################################################################\n",
    "def construct_mlp_layers(\n",
    "    n_hidden: int,\n",
    "    n_neurons: int, # 每个隐藏层中的神经元（或单元）数量。\n",
    "    act: Callable[[np.ndarray], np.ndarray], # 应用于每个隐藏层的激活函数。\n",
    "    output_dim: int,\n",
    "    residual_blocks: bool = True # 指示是否在MLP中使用残差连接。\n",
    ") -> list:\n",
    "    \"\"\"Make a list containing the layers of an MLP.\n",
    "\n",
    "    Args:\n",
    "        n_hidden: Number of hidden layers in the MLP.\n",
    "        n_neurons: Number of neurons per hidden layer.\n",
    "        act: Activation function.\n",
    "        output_dim: Dimension of the output.\n",
    "        residual_blocks: Whether or not to use residual blocks.\n",
    "    \"\"\"\n",
    "    layers = [] # 初始化一个空列表 layers，该列表将保存每层的配置。\n",
    "\n",
    "    resid_act = lambda x: x + act(x)\n",
    "    for layer in range(n_hidden):\n",
    "        ## construct layer\n",
    "        if layer == 0 or not residual_blocks:\n",
    "            # 如果是第一层（layer == 0）或不使用残差块（not residual_blocks），则该层配置为简单的线性变换后跟激活函数\n",
    "            layers = layers + [\n",
    "                    hk.Linear(n_neurons),\n",
    "                    act\n",
    "                ]\n",
    "        else:\n",
    "            # 对于后续层（当 residual_blocks 为 True时），每层配置为线性变换后跟残差激活函数\n",
    "            layers = layers + [\n",
    "                    hk.Linear(n_neurons),\n",
    "                    resid_act\n",
    "                ]\n",
    "\n",
    "\n",
    "    ## construct output layer\n",
    "    layers = layers + [hk.Linear(output_dim)] # 在构造所有隐藏层后，函数添加输出层，这是一个到所需输出维度的线性变换\n",
    "    return layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "# 该函数用于构建两个score网络或潜在能量的神经网络模型，适用于不涉及相互作用粒子的简化系统。\n",
    "def construct_score_network(\n",
    "    d: int,\n",
    "    n_hidden: int,\n",
    "    n_neurons: int,\n",
    "    act: Callable[[np.ndarray], np.ndarray],\n",
    "    residual_blocks: bool = True,\n",
    "    is_gradient: bool = False\n",
    ") -> Tuple[Callable, Callable]:\n",
    "    \"\"\"Construct a score network for a simpler system\n",
    "    that does not consist of interacting particles.\n",
    "\n",
    "    Args:\n",
    "        d: System dimension.\n",
    "        n_hidden: Number of hidden layers in the network.\n",
    "        n_neurons: Number of neurons per layer.\n",
    "        act: Activation function.\n",
    "        residual_blocks: Whether or not to use residual blocks.\n",
    "        is_gradient: Whether or not to compute the score as the gradient of a potential.\n",
    "    \"\"\"\n",
    "    output_dim = 1 if is_gradient else d \n",
    "    # 如果是梯度系统，就可以计算score + 能量， 就先定义能量，输出维度为 1；\n",
    "    # 否则只计算score，输出维度为 d。\n",
    "    net = lambda x: hk.Sequential(\n",
    "        construct_mlp_layers(n_hidden, n_neurons, act, output_dim, residual_blocks)\n",
    "    )(x)\n",
    "\n",
    "    if is_gradient:\n",
    "        # need to squeeze so the output of the potential is a scalar, rather than\n",
    "        # a 1-dimensional array.\n",
    "        potential = lambda x: np.squeeze(net(x)) # 定义 potential 函数，它通过 np.squeeze 确保输出为标量，适用于单输出网络。\n",
    "        score = jax.grad(potential)\n",
    "        return hk.without_apply_rng(hk.transform(score)), \\\n",
    "                hk.without_apply_rng(hk.transform(potential))\n",
    "    else:\n",
    "        return hk.without_apply_rng(hk.transform(net)), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nicholas M. Boffi\n",
    "7/29/22\n",
    "\n",
    "Loss functions for score-based transport modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import jax.numpy as np\n",
    "from typing import Tuple, Callable, Union, Optional\n",
    "from jax import jit, vmap, value_and_grad\n",
    "from jaxopt.linear_solve import solve_cg\n",
    "from functools import partial\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# 该函数用于计算一个初始潜在分布的梯度。\n",
    "# 在这个具体的例子中，潜在分布被假设为一个各向同性（isotropic）的高斯分布（正态分布），具有均值 mu0 和标准差 sig0。\n",
    "def grad_log_rho0(\n",
    "    sample: np.ndarray,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial potential. Assumed to be an isotropic Gaussian.\"\"\"\n",
    "    return -(sample - mu0) / sig0**2\n",
    "\n",
    "def rho0(\n",
    "    sample: np.ndarray,\n",
    "    sig0: float = 1,\n",
    "    mu0: np.ndarray = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial potential. Assumed to be an isotropic Gaussian.\"\"\"\n",
    "    sig0 = 1\n",
    "    mu0 = 0\n",
    "    return np.exp(-(sample - mu0)**2 / (2 * sig0**2))\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(4, 5, 7))\n",
    "def init_loss(\n",
    "    params: np.ndarray,\n",
    "    samples: np.ndarray,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray,\n",
    "    apply_score: Callable[[hk.Params, np.ndarray, Optional[float]], np.ndarray],\n",
    "    time_dependent: bool = False, # 指示模型是否具有时间依赖性。\n",
    "    frame_end: float = 0,\n",
    "    nt: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial loss, assuming access to \\nabla \\log \\rho0.\"\"\"\n",
    "\n",
    "    grad_log_rho_evals = vmap(\n",
    "        lambda sample:  D * grad_log_rho0(sample, sig0, mu0)\n",
    "    )(samples) # 使用 vmap 对 samples 中的每个样本应用 grad_log_rho0 函数，计算对数概率密度的梯度。\n",
    "    \n",
    "    score_evals = vmap(\n",
    "        lambda sample:   apply_score(params, sample)\n",
    "    )(samples)\n",
    "\n",
    "    # 函数 apply_score: 这个函数接受两个参数：params 和 sample。我们可以假设它根据某些参数 params 来评估或计算 sample 的“分数”或其他某种度量。\n",
    "    # Lambda 函数: lambda sample: apply_score(params, sample) 是一个匿名函数，它采用单个参数 sample，并调用 apply_score 函数，将 params 作为第一个参数。这里的 params 可能是在外部定义的，由于 lambda 函数的作用域，它能够捕获并使用这个外部变量。\n",
    "    # vmap 的使用: vmap 被用来自动向量化这个 lambda 函数。这意味着，vmap 将允许 apply_score 函数并行地处理 samples 数组中的每一个元素。\n",
    "    # 执行 vmap: vmap(...)(samples) 这部分代码实际上是在调用 vmap 返回的向量化函数，并将 samples 作为输入。这里 samples 应该是一个数组或类似数组的结构，每个元素都是一个可以被 apply_score 处理的 \"sample\"。\n",
    "    # 结果 score_evals: 最终的结果 score_evals 将是一个数组，其中包含对 samples 中每个样本应用 apply_score 函数得到的结果\n",
    "\n",
    "    @jax.jit\n",
    "    def levypart(sample):\n",
    "        sample = np.array(sample)   # 应至少为一维数组\n",
    "\n",
    "        # 计算所有 lambdaj[j] * ri[i] 的组合\n",
    "        lambdaj_ri_matrix = lambdaj[:, None] * ri[None, :]  # [20, 50]\n",
    "        lambdaj_ri_matrix = np.expand_dims(lambdaj_ri_matrix, axis=2)\n",
    "        # 扩展 sample\n",
    "        expanded_samples = sample - lambdaj_ri_matrix  # 假设 sample 是 [k,] 形状\n",
    "\n",
    "        all_scores = (jax.vmap(jax.vmap(rho0))(expanded_samples)).squeeze()\n",
    "\n",
    "        scores_sumj = np.mean(all_scores, axis=0)\n",
    "        # print('scores_sumj * ri * N_ri * 10 * sig_jump',(scores_sumj * ri * N_ri * 10 * sig_jump).shape,scores_sumj.shape)\n",
    "        return np.mean(scores_sumj * ri * N_ri * 7 * sig_jump) / rho0(sample)\n",
    "\n",
    "        \n",
    "\n",
    "    LEVYpart = vmap(levypart, in_axes=0)(samples)\n",
    "    # print('LEVYpart.shape,samples.shape',grad_log_rho_evals.shape,LEVYpart.shape,samples.shape)\n",
    "    score = np.sum(   ( score_evals - (grad_log_rho_evals - LEVYpart) )**2) \\\n",
    "            / np.sum((grad_log_rho_evals-LEVYpart)**2)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@jit\n",
    "def compute_grad_norm(\n",
    "    grads: hk.Params\n",
    ") -> float:\n",
    "    \"\"\" Computes the norm of the gradient, where the gradient is input\n",
    "    as an hk.Params object (treated as a PyTree).\"\"\"\n",
    "    flat_params = ravel_pytree(grads)[0]\n",
    "    # ravel_pytree 是一个函数，用于将 PyTree 结构的 grads 扁平化成一个一维数组。这使得可以对整个梯度向量进行数学操作。\n",
    "    # [0] 是因为 ravel_pytree 返回一个元组，其中第一个元素是扁平化的数组，第二个元素通常是用于以后恢复原始结构的模板。\n",
    "    return np.linalg.norm(flat_params) / np.sqrt(flat_params.size)\n",
    "    # np.linalg.norm(flat_params) 计算扁平化梯度数组的 L2 范数（欧几里得范数）。\n",
    "    # flat_params.size 获取扁平化数组的元素个数（即梯度向量的维度）。\n",
    "    # 通过除以 np.sqrt(flat_params.size)，函数返回的是规范化的范数，这样做是为了使范数的计算与参数的数量无关，有助于更公平地比较不同大小模型的梯度大小。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(2, 3))\n",
    "def update(\n",
    "    params: hk.Params, # 待优化的网络参数，类型为 hk.Params，这是 Haiku 库中用于表示参数的数据结构。\n",
    "    opt_state: optax.OptState, # 优化器的状态，类型为 optax.OptState，这是 Optax 库中用于存储优化器状态（如动量等）的数据结构。\n",
    "    opt: optax.GradientTransformation, # 优化器本身，类型为 optax.GradientTransformation，这是 Optax 库中用于定义梯度更新规则的对象。\n",
    "    # opt = optax.adabelief(self.init_learning_rate)\n",
    "    loss_func: Callable[[hk.Params], float],\n",
    "    loss_func_args: Tuple = tuple(), # 传递给损失函数的额外参数，用于支持更灵活的函数签名。\n",
    ") -> Tuple[hk.Params, optax.OptState, float, hk.Params]:\n",
    "    \"\"\"Update the neural network.\n",
    "\n",
    "    Args:\n",
    "        params: Parameters to optimize over.\n",
    "        opt_state: State of the optimizer.\n",
    "        opt: Optimizer itself.\n",
    "        loss_func: Loss function for the parameters.\n",
    "    \"\"\"\n",
    "    loss_value, grads = value_and_grad(loss_func)(params, *loss_func_args)\n",
    "    \n",
    "    # value_and_grad 是一个函数，它接受另一个函数（在这里是 loss_func），并返回一个新的函数，这个新函数同时计算原函数的值和关于参数的梯度。\n",
    "    # 在这行代码中，使用解包操作符 * 将 loss_func_args 中的元素作为额外参数传递给 loss_func。\n",
    "    updates, opt_state = opt.update(grads, opt_state, params=params) # 使用优化器的 update 方法计算参数更新和新的优化器状态。这通常基于当前梯度和先前的优化器状态进行计算。\n",
    "    new_params = optax.apply_updates(params, updates) # 使用 optax.apply_updates，将计算得到的更新应用到当前参数上，产生新的参数。\n",
    "    return new_params, opt_state, loss_value, grads # 返回更新后的参数、新的优化器状态、计算得到的损失值以及参数的梯度。这些输出对于进一步的分析和调试训练过程非常有用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nicholas M. Boffi\n",
    "7/29/22\n",
    "\n",
    "Updates to parameters and particles for score-based transport modeling.\n",
    "\"\"\"\n",
    "\n",
    "from jax import jit, vmap\n",
    "import jax\n",
    "from functools import partial\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import haiku as hk\n",
    "from typing import Callable, Tuple, Union\n",
    "# from tqdm.auto import tqdm as tqdm\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(5, 6)) \n",
    "# 参数 static_argnums 指定第 5 和第 6 个参数（从 0 开始计数，即 forcing 和 apply_score）在函数调用时应该是静态的，\n",
    "# 这意味着这些函数的引用在多次调用中不应改变，以便于 JIT 编译器优化。\n",
    "# 执行一个前向欧拉步骤来更新粒子的位置。\n",
    "def update_particles(\n",
    "    particle_pos: np.ndarray,\n",
    "    t: float,\n",
    "    params: hk.Params,\n",
    "    D: Union[np.ndarray, float], # 扩散系数，可以是一个数值或者一个与粒子位置同形状的数组。 Union 是用来表现变量可能拥有的不同类型的一个联合。\n",
    "    dt: float,\n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray], # 根据给定的粒子位置和时间计算外力。\n",
    "    apply_score: Callable[[hk.Params, np.ndarray], np.ndarray], # 用模型参数和粒子位置计算score.\n",
    "    mask: np.ndarray = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Take a forward Euler step and update the particles.\"\"\"\n",
    "    \n",
    "    if mask is not None:\n",
    "        score_term = -mask * apply_score(params, particle_pos)\n",
    "    else:\n",
    "        score_term = -apply_score(params, particle_pos)\n",
    "\n",
    "    return particle_pos + dt*(forcing(particle_pos, t) + score_term)\n",
    "    # 它将原始位置与由外力项 forcing(particle_pos, t) 和得分项 score_term 的和乘以时间步长 dt 的结果相加。\n",
    "    # 这是一个典型的前向欧拉积分步骤，用于将粒子向其动力学方向移动。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(5, 6))\n",
    "# 使用 Euler-Maruyama 方法来更新粒子位置，这通常涉及到一些随机性（噪声）的引入。\n",
    "def update_particles_EM(\n",
    "    particle_pos: np.ndarray,\n",
    "    t: float,\n",
    "    D_sqrt: Union[np.ndarray, float], # 扩散系数的平方根，可以是一个数值或者一个与粒子位置数组相同形状的数组。\n",
    "    dt: float,\n",
    "    key: np.ndarray, # 用于生成随机数的密钥，此参数与 JAX 的随机数生成机制相关。\n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray],\n",
    "    noisy: bool = True,\n",
    "    mask: np.ndarray = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Take a step forward via Euler-Maruyama to update the particles.\"\"\"\n",
    "\n",
    "    if noisy:\n",
    "        noise = np.sqrt(2*dt) * jax.random.normal(key, shape=particle_pos.shape)\n",
    "        if mask is not None:\n",
    "            brownian = -D_sqrt * mask * noise\n",
    "        else:\n",
    "            brownian = -D_sqrt * noise\n",
    "\n",
    "        return particle_pos + dt*forcing(particle_pos, t) + brownian\n",
    "    else:\n",
    "        return particle_pos + dt*forcing(particle_pos, t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# 使用 Euler-Maruyama 方法从一组初始条件（x0s）生成随机轨迹\n",
    "def rollout_EM_trajs(\n",
    "    x0s: np.ndarray,\n",
    "    nsteps: int,\n",
    "    t0: float,\n",
    "    dt: float,\n",
    "    key: np.ndarray,\n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray],\n",
    "    D_sqrt: Union[np.ndarray, float],\n",
    "    noisy: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Given a set of initial conditions, create a stochastic trajectory \n",
    "    via Euler-Maruyama. Useful for constructing a baseline against which to compare\n",
    "    the moments.\n",
    "\n",
    "    Args:\n",
    "    ------\n",
    "    x0s: Initial condition. Dimension = n x d where n is the number of samples \n",
    "         and d is the dimension of the system.\n",
    "    nsteps: Number of steps of Euler-Maruyama to take.\n",
    "    t0: initial time.\n",
    "    dt: Timestep.\n",
    "    key: jax PRNG key.\n",
    "    forcing: Forcing to apply to the particles.\n",
    "    D_sqrt: Square root of the diffusion matrix.\n",
    "    \"\"\"\n",
    "    n, d = x0s.shape\n",
    "    trajs = onp.zeros((nsteps+1, n, d)) # 创建一个用于存储粒子轨迹的零数组。数组的形状为 (nsteps+1, n, d)，即存储从初始时间到最后一个时间步的粒子位置。\n",
    "    trajs[0] = x0s\n",
    "    step_sample = \\\n",
    "            lambda sample, t, key: update_particles_EM(sample, t, D_sqrt, \n",
    "                                                       dt, key, forcing, noisy)\n",
    "    step_samples = vmap(step_sample, in_axes=(0, None, 0), out_axes=0)\n",
    "\n",
    "    \n",
    "    for curr_step in tqdm(range(nsteps)):\n",
    "        t = t0 + curr_step*dt\n",
    "        keys = jax.random.split(key, num=n) # 使用 JAX 的随机功能，将密钥分割成 n 个新的随机密钥，以便为每个粒子生成随机数。\n",
    "        trajs[curr_step+1] = step_samples(trajs[curr_step], t, keys)\n",
    "        key = keys[-1] # 更新使用的主密钥为生成的密钥中的最后一个，以便在下一次迭代中使用。\n",
    "\n",
    "    return trajs, key # 返回存储了所有时间步粒子位置的轨迹数组和最后的密钥状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nicholas M. Boffi\n",
    "8/4/22\n",
    "\n",
    "Training loops for score-based transport modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from jax import jit, vmap\n",
    "from jax.tree_util import tree_map\n",
    "import jax\n",
    "from functools import partial\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import haiku as hk\n",
    "from typing import Callable, Tuple, Union\n",
    "# import losses\n",
    "# import updates\n",
    "import optax\n",
    "import dill as pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from jaxlib.xla_extension import Device\n",
    "\n",
    "Time = float\n",
    "\n",
    "\n",
    "def fit_initial_condition(\n",
    "    n_max_opt_steps: int, # 最大优化步骤数。\n",
    "    ltol: float, # 损失值的容忍度，当损失值低于此阈值时优化停止。\n",
    "    params: hk.Params, \n",
    "    sig0: float, # 目标初始条件的标准差。\n",
    "    mu0: np.ndarray,\n",
    "    score_network: Callable[[hk.Params, np.ndarray], np.ndarray],\n",
    "    opt: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    "    samples: np.ndarray,\n",
    "    time_dependent: bool = False,\n",
    "    frame_end: float = 0,\n",
    "    nt: int = 0\n",
    ") -> hk.Params:\n",
    "    \"\"\"Fit the score for the initial condition.\n",
    "\n",
    "    Args:\n",
    "        n_opt_steps: Number of optimization steps before the norm of the gradient \n",
    "                     is checked.\n",
    "        gtol: Tolerance on the norm of the gradient.\n",
    "        ltol: Tolerance on the relative error.\n",
    "        params: Parameters to optimize over.\n",
    "        sig0: Standard deviation of the target initial condition.\n",
    "        mu0: Mean of the target initial condition.\n",
    "        score_network: Function mapping parameters and a sample to the network output.\n",
    "        opt: Optimizer.\n",
    "        opt_state: State of the optimizer.\n",
    "        samples: Samples to optimizer over.\n",
    "    \"\"\"\n",
    "    # 在 haiku 中，你首先定义一个模型的前向传播函数，并使用 hk.transform 转换这个函数，从而生成一个 Transformed 对象。这个对象包含两个主要的方法：init 和 apply。\n",
    "\n",
    "    # init：用于初始化模型参数。\n",
    "    # apply：用于在给定输入和参数的情况下运行模型。\n",
    "    \n",
    "    apply_score = score_network.apply # 这行代码提取 score_network 的 apply 方法，执行前向传递。\n",
    "    loss_func = lambda params: \\\n",
    "            init_loss(params, samples, sig0, mu0, apply_score, \n",
    "                             time_dependent, frame_end, nt)\n",
    "\n",
    "    loss_val = np.inf\n",
    "    try:\n",
    "        with tqdm(range(n_max_opt_steps)) as pbar:\n",
    "            pbar.set_description(\"Initial optimization\")\n",
    "            for curr_step in pbar:\n",
    "                try:\n",
    "                    params, opt_state, loss_val, grads = update(params, opt_state, opt, loss_func)\n",
    "                    pbar.set_postfix(loss=loss_val)\n",
    "                    if loss_val < ltol:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error at step {curr_step}: {e}\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing the progress bar: {e}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drifts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nicholas M. Boffi\n",
    "7/29/22\n",
    "\n",
    "Drift terms for score-based transport modeling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from jax import vmap\n",
    "from jax.lax import stop_gradient\n",
    "import jax.numpy as np\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "def hyf_exapmle1(\n",
    "    x: np.ndarray,\n",
    "    t: float,\n",
    "    # gamma: float\n",
    "    kappa: float,\n",
    "    eta: float\n",
    ") -> np.ndarray:\n",
    "    del t\n",
    "    return (kappa * (eta - x) )\n",
    "\n",
    "############################################################################################################################\n",
    "def hyf_exapmle3(\n",
    "    x: np.ndarray,\n",
    "    t: float,\n",
    "    # gamma: float\n",
    "    kappa: float,\n",
    "    eta: float\n",
    ") -> np.ndarray:\n",
    "    del t\n",
    "    return (kappa * x - eta * x**3 )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "# active_swimmer 函数是一个模拟运动中的粒子的例子，其中粒子受到非线性的势能以及阻尼力的影响。这个模型可以用于描述例如在流体中的微观游泳者（swimmer）等动力学系统。\n",
    "# 这里 -x**3 可以视为一个关于位置 x 的非线性势能导数的反向，v 是当前速度。 \n",
    "# -gamma*v 这一项代表了一个与速度成比例且方向相反的阻尼力，其中 gamma 是阻尼系数。\n",
    "def active_swimmer(\n",
    "    xv: np.ndarray,\n",
    "    t: float,\n",
    "    gamma: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Active swimmer example.\"\"\"\n",
    "    del t\n",
    "    x, v = xv\n",
    "    print(x.shape,xv.shape,(np.array([-x**3 + v, -gamma*v])) .shape)\n",
    "    return np.array([-x**3 + v, -gamma*v])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBTM-SIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base class for score-based transport modeling.\n",
    "\n",
    "Nicholas M. Boffi\n",
    "10/27/22\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple, Union\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as onp\n",
    "from jaxlib.xla_extension import Device\n",
    "import optax\n",
    "\n",
    "# import networks\n",
    "# import rollouts\n",
    "\n",
    "State = onp.ndarray\n",
    "Time = float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "# @dataclass 是一个装饰器，用于自动化一些用于存储数据的类的常见模式。这个装饰器来自标准库中的 dataclasses 模块，主要目的是让类更加简洁易读。\n",
    "class SBTMSim:\n",
    "    \"\"\"\n",
    "    Base class for all SBTM simulations.\n",
    "    Contains simulation parameters common to all SBTM approaches.\n",
    "    \"\"\"\n",
    "    # initial condition fitting\n",
    "    n_max_init_opt_steps: int\n",
    "    init_learning_rate: float\n",
    "    init_ltol: float\n",
    "    sig0: float\n",
    "    mu0: onp.ndarray\n",
    "\n",
    "    # system parameters\n",
    "    drift: Callable[[State, Time], State]\n",
    "    force_args: Tuple\n",
    "    amp: Callable[[Time], float]\n",
    "    freq: float\n",
    "    dt: float\n",
    "    D: onp.ndarray\n",
    "    D_sqrt: onp.ndarray\n",
    "    n: int\n",
    "    d: int\n",
    "    N: int\n",
    "\n",
    "    # timestepping\n",
    "    ltol: float\n",
    "    gtol: float\n",
    "    n_opt_steps: int\n",
    "    learning_rate: float\n",
    "\n",
    "    # network parameters\n",
    "    n_hidden: int\n",
    "    n_neurons: int\n",
    "    act: Callable[[State], State]\n",
    "    residual_blocks: bool\n",
    "    interacting_particle_system: bool\n",
    "\n",
    "    # general simulation parameters\n",
    "    key: onp.ndarray\n",
    "    params_list: list\n",
    "    all_samples: dict\n",
    "\n",
    "    # output information\n",
    "    output_folder: str\n",
    "    output_name: str\n",
    "\n",
    "\n",
    "    def __init__(self, data_dict: dict) -> None:\n",
    "        self.__dict__ = data_dict.copy()\n",
    "    # 初始化方法，直接将传入的字典复制到类的属性中。\n",
    "\n",
    "    def initialize_forcing(self) -> None:\n",
    "        self.forcing = lambda x, t: self.drift(x, t, *self.force_args)\n",
    "    # 初始化外力函数。这里 forcing 是一个 lambda 函数，接受状态 x 和时间 t，调用定义的 drift 函数。\n",
    "\n",
    "    def initialize_network_and_optimizer(self) -> None:\n",
    "        \"\"\"Initialize the network parameters and optimizer.\"\"\"\n",
    "        if self.interacting_particle_system:\n",
    "            self.score_network, self.potential_network = \\\n",
    "                    construct_interacting_particle_system_network(\n",
    "                            self.n_hidden, \n",
    "                            self.n_neurons, \n",
    "                            self.N, \n",
    "                            self.d, \n",
    "                            self.act, \n",
    "                            self.residual_blocks\n",
    "                        )\n",
    "\n",
    "            example_x = onp.zeros(self.N*self.d) # 初始化一个示例输入数组 example_x，用于网络初始化。其大小是粒子总数乘以维度。\n",
    "        else:\n",
    "            self.score_network, self.potential_network= \\\n",
    "                construct_score_network(\n",
    "                    self.d,\n",
    "                    self.n_hidden,\n",
    "                    self.n_neurons,\n",
    "                    self.act,\n",
    "                    is_gradient=False\n",
    "                )\n",
    "            \n",
    "            example_x = onp.zeros(self.d) # 对于非交互粒子系统，初始化的示例输入数组 example_x 只与维度有关。\n",
    "        self.key, sk = jax.random.split(self.key) # 使用 JAX 的随机数生成功能，分割随机数种子 key。这是为了确保随机性的可控制和重现性。\n",
    "        init_params = self.score_network.init(self.key, example_x) # 使用网络的初始化方法（这通常是由 Haiku 提供的）和示例输入 examplex_x 来初始化网络参数。 (基于这个样本的尺寸来确定你各层的参数尺寸)\n",
    "        self.params_list = [init_params] # 将初始化的参数存储到 params_list 中。\n",
    "\n",
    "\n",
    "        network_size = jax.flatten_util.ravel_pytree(init_params)[0].size # 使用了 JAX 的 jax.flatten_util.ravel_pytree 函数把复杂的参数树（pytree）“展平”成一个简单的一维数组\n",
    "        print(f'Number of parameters: {network_size}')\n",
    "        print(f'Number of parameters needed for overparameterization: ' \\\n",
    "                + f'{self.n*example_x.size}')\n",
    "\n",
    "        # set up the optimizer\n",
    "        self.opt = optax.radam(self.learning_rate) # 使用 Optax 库中的 RAdam（Rectified Adam）优化器初始化优化器，并设置学习率。\n",
    "        self.opt_state = self.opt.init(init_params) # 使用优化器的初始化方法和网络参数初始化优化器的状态。\n",
    "\n",
    "        # set up batching for the score\n",
    "        self.batch_score = jax.vmap(self.score_network.apply, in_axes=(None, 0)) # 使用 JAX 的 vmap 函数设置批量评分功能。这允许并行处理多个输入，提高计算效率。\n",
    "\n",
    "    def fit_init(self, cpu: Device, gpu: Device) -> None:\n",
    "        \"\"\"Fit the initial condition.\"\"\"\n",
    "        # draw samples\n",
    "        samples_shape = (self.n, self.N*self.d) # 确定样本的形状，self.n 是样本数量，self.N*self.d 表示每个样本的维度。\n",
    "        init_samples = self.sig0*onp.random.randn(*samples_shape) + self.mu0[None, :] # init_samples 通过从正态分布中抽取样本并按照特定的均值 (self.mu0) 和标准差 (self.sig0) 进行调整来生成初始样本。\n",
    "\n",
    "        # set up optimizer\n",
    "        init_params = jax.device_put(self.params_list[0], gpu) # init_params 从 params_list 中获取第一组参数，并使用 jax.device_put 确保这些参数被加载到 gpu 设备上。\n",
    "        opt = optax.adabelief(self.init_learning_rate) # 初始化一个 AdaBelief 优化器 (optax.adabelief)，此优化器适用于深度学习应用中的稳健优化。\n",
    "        opt_state = opt.init(init_params)\n",
    "\n",
    "        init_params = fit_initial_condition(\n",
    "                            self.n_max_init_opt_steps,\n",
    "                            self.init_ltol,\n",
    "                            init_params,\n",
    "                            self.sig0,\n",
    "                            self.mu0,\n",
    "                            self.score_network,\n",
    "                            opt,\n",
    "                            opt_state,\n",
    "                            init_samples\n",
    "                        )\n",
    "\n",
    "\n",
    "        self.params_list = [jax.device_put(init_params, device=cpu)] # 更新 self.params_list，其中包含了优化后的参数，这次使用 jax.device_put 确保参数被转移到 cpu 设备。\n",
    "        self.all_samples = {'SDE': [init_samples], 'learned': [init_samples]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBTM-SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for sequential SBTM.\n",
    "\n",
    "Nicholas M. Boffi\n",
    "10/27/22\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import numpy as onp\n",
    "# from tqdm.auto import tqdm as tqdm\n",
    "import dill as pickle\n",
    "import time\n",
    "from jaxlib.xla_extension import Device\n",
    "from haiku import Params\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# import sbtm_sim\n",
    "# import updates\n",
    "# import losses\n",
    "# from sbtm_analysis import compute_entropy_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequentialSBTM(SBTMSim): # 它继承自一个模拟框架 sbtm_sim.SBTMSim\n",
    "    n_time_steps: int\n",
    "    use_SDE: bool\n",
    "    use_ODE: bool\n",
    "    save_fac: int \n",
    "    store_fac: int # 整数，控制数据保存和存储的频率。\n",
    "    means: dict\n",
    "    covs: dict # 字典，用于存储模拟过程中计算的均值和协方差。\n",
    "    entropies: list # 列表，用于记录熵的变化。\n",
    "    mask: np.ndarray\n",
    "\n",
    "\n",
    "    def setup_loss(self):\n",
    "        \"\"\"Define the loss function. \"\"\"\n",
    "        raise NotImplementedError(\"Please implement in the inheriting class.\") # 类比：强制任何继承 Animal 的类（如 Dog 和 Cat）实现自己版本的 make_sound() 方法， 否则就报错\n",
    "\n",
    "\n",
    "    \n",
    "    def setup_loss_fn_args(self, gpu: Device) -> Tuple:\n",
    "        \"\"\"Define the arguments to the loss function other than parameters.\"\"\"\n",
    "        raise NotImplementedError(\"Please implement in the inheriting class.\")\n",
    "        \n",
    "\n",
    "    def setup_batched_steppers(self):\n",
    "        \"\"\"Construct convenience functions to step the particles.\"\"\"\n",
    "        self.step_learned = vmap(\n",
    "                lambda params, t, sample: update_particles(\n",
    "                    sample, \n",
    "                    t, \n",
    "                    params, \n",
    "                    self.D, \n",
    "                    self.dt,  \n",
    "                    self.forcing, \n",
    "                    self.score_network.apply,\n",
    "                    self.mask\n",
    "                ),\n",
    "                in_axes=(None, None, 0),\n",
    "                out_axes=0\n",
    "        )\n",
    "\n",
    "        self.step_SDE = vmap(\n",
    "                lambda t, sample, key: update_particles_EM(\n",
    "                    sample, \n",
    "                    t, \n",
    "                    self.D_sqrt, \n",
    "                    self.dt, \n",
    "                    key,  \n",
    "                    self.forcing,\n",
    "                    True,\n",
    "                    self.mask\n",
    "                ),\n",
    "                in_axes=(None, 0, 0),\n",
    "                out_axes=0\n",
    "        )\n",
    "\n",
    "    # 更新并保存 SDE 和 ODE 样本\n",
    "    def step_samples(\n",
    "        self,\n",
    "        step,\n",
    "        params: Params,\n",
    "        t: float,\n",
    "        samples: np.ndarray,\n",
    "        SDE_samples: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Step and save both the SDE and ODE samples.\"\"\"\n",
    "        # step learned and SDE particles\n",
    "        samples = self.step_learned(params, t, samples)\n",
    "        keys = jax.random.split(self.key, num=self.n)\n",
    "        SDE_samples = self.step_SDE(t, SDE_samples, keys)\n",
    "        # print('!!!!!!!!!!SDE_samples.s',SDE_samples.shape,keys.shape)\n",
    "        self.key = keys[-1]\n",
    "\n",
    "        # save new samples\n",
    "        if (step+1) % self.store_fac == 0:\n",
    "            self.all_samples['learned'].append(onp.array(samples))\n",
    "            self.all_samples['SDE'].append(onp.array(SDE_samples))\n",
    "\n",
    "        return samples, SDE_samples\n",
    "\n",
    "\n",
    "    def setup_learning_samples(\n",
    "        self,\n",
    "        samples: np.ndarray,\n",
    "        SDE_samples: np.ndarray\n",
    "    ) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Set up samples to optimize over.\"\"\"\n",
    "        if self.use_ODE and self.use_SDE:\n",
    "            opt_samples = (np.vstack((samples, SDE_samples)),)\n",
    "        elif self.use_ODE:\n",
    "            opt_samples = (samples,)\n",
    "        elif self.use_SDE:\n",
    "            opt_samples = (SDE_samples,)\n",
    "        else:\n",
    "            raise ValueError('Need to specify learning from ODE or SDE.')\n",
    "\n",
    "        return opt_samples\n",
    "\n",
    "\n",
    "    def compute_moments_and_entropy_production(\n",
    "        self,\n",
    "        params: Params,\n",
    "        t: float,\n",
    "        samples: np.ndarray, \n",
    "        SDE_samples: np.ndarray\n",
    "    ) -> None:\n",
    "        ## entropy\n",
    "        self.entropies.append(\n",
    "                compute_entropy_rate(samples, t, params, self.D, self.forcing, \n",
    "                                     self.score_network, noise_free=False, div=True)\n",
    "            )\n",
    "\n",
    "        ## moments\n",
    "        self.means['SDE'].append(np.mean(SDE_samples, axis=0))\n",
    "        self.covs['SDE'].append(np.cov(SDE_samples, rowvar=False))\n",
    "\n",
    "        self.means['learned'].append(np.mean(samples, axis=0))\n",
    "        self.covs['learned'].append(np.cov(samples, rowvar=False))\n",
    "\n",
    "\n",
    "    def solve_fpe_sequential(self, cpu: Device, gpu: Device):\n",
    "        # set up some convenience variables\n",
    "        self.setup_loss()\n",
    "        self.setup_batched_steppers()\n",
    "        nt = len(self.params_list) - 1\n",
    "\n",
    "        # move needed data to the GPU for speed.\n",
    "        params = jax.device_put(self.params_list[-1], gpu) # 将参数列表中的最后一个参数集迁移到 GPU 上，以加速计算。\n",
    "        opt_state = jax.device_put(self.opt_state, gpu)\n",
    "        samples = jax.device_put(self.all_samples['learned'][-1], gpu) # 将学习得到的样本的最后一批迁移到 GPU。\n",
    "        SDE_samples = jax.device_put(self.all_samples['SDE'][-1], gpu) # 将 SDE 样本的最后一批迁移到 GPU。\n",
    "\n",
    "        # store the initial moments and entropy.\n",
    "        # if nt == 0: # 如果 nt 是 0（即是第一次执行），则计算初始时刻的统计矩和熵产生。\n",
    "        #     self.compute_moments_and_entropy_production(params, t=0, \n",
    "        #                                                 samples=samples, \n",
    "        #                                                 SDE_samples=SDE_samples)\n",
    "\n",
    "        ## output progress bar\n",
    "        with tqdm(range(self.n_time_steps)) as pbar: #使用 tqdm 库创建一个进度条，用于显示时间步进的进度。\n",
    "            for step in pbar:\n",
    "                t = (nt*self.store_fac + step)*self.dt\n",
    "                pbar.set_description(f\"Dynamics: t={t:.3f}\") # 遍历每个时间步。计算当前的时间 t，并更新进度条的描述。\n",
    "\n",
    "                samples, SDE_samples = self.step_samples(step, params, t, samples, SDE_samples) # 在当前时间步上，更新样本和 SDE 样本。\n",
    "                opt_samples = self.setup_learning_samples(samples, SDE_samples)\n",
    "\n",
    "                ## perform the optimization\n",
    "                loss_value, grad_norm = np.inf, np.inf\n",
    "                num_steps_taken = 0\n",
    "                while (grad_norm > self.gtol):\n",
    "                    for curr_opt_step in range(self.n_opt_steps): # 循环执行优化步骤，计算损失函数的参数，记录时间。\n",
    "                        loss_func_args = opt_samples + self.setup_loss_fn_args(gpu)\n",
    "                        # print(loss_func_args)\n",
    "                        start_time = time.time()\n",
    "                        params, opt_state, loss_value, grads \\\n",
    "                                = update(\n",
    "                                        params, \n",
    "                                        opt_state, \n",
    "                                        self.opt, \n",
    "                                        self.loss_func, \n",
    "                                        loss_func_args\n",
    "                                    )\n",
    "                        end_time = time.time()\n",
    "\n",
    "                    grad_norm = compute_grad_norm(grads)\n",
    "                    pbar.set_postfix(\n",
    "                        loss=loss_value, ltol=self.ltol,\n",
    "                        grad_norm=grad_norm, gtol=self.gtol,\n",
    "                        step_time=end_time-start_time\n",
    "                    ) # 更新进度条的后缀信息。\n",
    "\n",
    "                # if (step+1) % self.store_fac == 0:\n",
    "                #     self.params_list.append(jax.device_put(params, cpu))\n",
    "                #     self.compute_moments_and_entropy_production(params, t, samples, SDE_samples)\n",
    "                # 如果达到存储频率，将参数迁回 CPU 并保存，计算当前参数下的统计矩和熵产。\n",
    "                if (step+1) % self.save_fac == 0:\n",
    "                    self.save_data()\n",
    "                # 如果达到保存频率，保存数据。\n",
    "        self.save_data()\n",
    "\n",
    "\n",
    "    def save_data(self):\n",
    "        data = vars(self).copy()\n",
    "        # vars(self) 返回一个包含对象 self 实例的所有属性和值的字典。\n",
    "        # 调用 .copy() 方法创建这个字典的浅拷贝，以确保在保存数据的过程中原始数据不被更改。\n",
    "        \n",
    "        pickle.dump(data, open(f'{self.output_folder}/{self.output_name}', 'wb'))\n",
    "        # 使用 Python 的 pickle 模块来序列化 data 对象。pickle.dump 函数用于将对象序列化并写入一个文件。\n",
    "        # open(f'{self.output_folder}/{self.output_name}', 'wb') 打开一个文件用于写入，路径由 self.output_folder 和 self.output_name 属性定义，'wb' 表示“写入二进制”模式。\n",
    "        # 序列化的数据被写入指定的文件中，文件路径由对象的 output_folder 和 output_name 属性组成。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenoisingSequentialSBTM(SequentialSBTM):\n",
    "    noise_fac: float # noise_fac 是一个类属性，用于在计算去噪损失时缩放噪声。它是一个决定添加到样本中噪声幅度的因子。\n",
    "\n",
    "    def setup_loss(self):\n",
    "        # def score_for_sample(params, sample):\n",
    "        #     # 扩展 sample 和 N_ri 以适应广播\n",
    "        #     weighted_scores = 0\n",
    "        #     for i in range(50):\n",
    "        #         for j in range (20):\n",
    "        #             expanded_sample = sample + lambdaj[j] * ri[i]\n",
    "        #             all_scores = self.score_network.apply(params, expanded_sample)\n",
    "        #             # 与 N_ri 相乘并计算总分\n",
    "        #             weighted_scores += all_scores * N_ri[i]\n",
    "        #     return weighted_scores\n",
    "        @jax.jit\n",
    "        def score_for_sample(params, sample):\n",
    "            sample = np.array(sample)   # 应至少为一维数组\n",
    "\n",
    "            # 计算所有 lambdaj[j] * ri[i] 的组合\n",
    "            lambdaj_ri_matrix = lambdaj[:, None] * ri[None, :]  # [20, 50]\n",
    "            lambdaj_ri_matrix = np.expand_dims(lambdaj_ri_matrix, axis=2)\n",
    "            # 扩展 sample\n",
    "            expanded_samples = sample + lambdaj_ri_matrix  # 假设 sample 是 [k,] 形状\n",
    "\n",
    "            # 使用 vmap 向量化评分网络的应用\n",
    "            def apply_network(exp_sample):\n",
    "                return self.score_network.apply(params, exp_sample)\n",
    "\n",
    "            all_scores = (jax.vmap(jax.vmap(apply_network))(expanded_samples)).squeeze()\n",
    "            \n",
    "            print('print(all_scores.shape,ri.shape)',all_scores.shape)\n",
    "            # 加权得分\n",
    "            scores_sumj = np.mean(all_scores, axis=0)\n",
    "            # print('print(scores_sumj.shape)',scores_sumj.shape,ri.shape, (scores_sumj * ri).shape, (scores_sumj * ri_reshaped).shape)\n",
    "            # print('np.mean(scores_sumj * ri)=', np.mean(scores_sumj * ri),(np.mean(scores_sumj * ri).shape))\n",
    "            print('np.squeeze(np.mean(scores_sumj * ri * N_ri * 6 * sig_jump))',(scores_sumj * ri * N_ri * 6 * sig_jump).shape)\n",
    "            return np.squeeze(np.mean(scores_sumj * ri * N_ri * 7 * sig_jump))\n",
    "            \n",
    "        def sample_denoising_loss(\n",
    "            params: Params,\n",
    "            sample: np.ndarray,\n",
    "            noise: np.ndarray,\n",
    "        ) -> float: # 这是一个定义为计算单个样本的去噪损失的嵌套函数。它使用模型的参数 (params)、数据集的样本 (sample) 和一个噪声数组 (noise)。\n",
    "            \"\"\"\n",
    "            Compute the denoising loss on a single sample, using antithetic sampling\n",
    "            over the noise for variance reduction.\n",
    "            \"\"\"\n",
    "            loss = 0\n",
    "            for sign in [-1, 1]:\n",
    "                perturbed_sample = sample + self.noise_fac*sign*noise\n",
    "                score = self.mask*self.score_network.apply(params, perturbed_sample)\n",
    "                loss += np.sum(self.noise_fac*score**2 + (2*D) *sign*score*noise)\n",
    "            # 使用对偶采样计算损失，通过添加和减去噪声 (sign 为 -1 和 1)，以减少损失估计的方差。\n",
    "            # perturbed_sample 是通过噪声（按 noise_fac 缩放）调整的样本。\n",
    "            # score 通过将得分网络应用于扰动样本来计算，并通过 self.mask 进行掩码处理。\n",
    "            # 损失计算包括得分的平方项和得分与噪声的交叉项。\n",
    "\n",
    "            return np.squeeze(loss / 2) # 对两个对偶样本的单个样本损失进行平均，并压缩以移除任何多余的维度。\n",
    "\n",
    "        self.loss_func = lambda params, samples, noise: np.mean(vmap(sample_denoising_loss, in_axes=(None, 0, 0))(params, samples, noise))  +2 * self.noise_fac* np.mean(vmap(score_for_sample, in_axes=(None, 0))(params, samples))\n",
    "        print(self.loss_func)\n",
    "        \n",
    "        # self.loss_func 定义为一个 lambda 函数，计算一批样本的平均损失。 使用 vmap 将 sample_denoising_loss 向量化到样本和噪声的批次上，允许并行计算损失。\n",
    "\n",
    "        if self.use_SDE and self.use_ODE:\n",
    "            self.n_train_samples = 2*self.n\n",
    "        else:\n",
    "            self.n_train_samples = self.n\n",
    "\n",
    "\n",
    "    def setup_loss_fn_args(self, gpu: Device) -> Tuple:\n",
    "        \"\"\"Set up noise arguments for the loss function. \"\"\"\n",
    "        noises = onp.random.randn(self.n_train_samples, self.d*self.N)\n",
    "        loss_func_args = (jax.device_put(noises, gpu),)\n",
    "        return loss_func_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 16:37:04.359684: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.68). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674621\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "sig_jump = 2/48\n",
    "mu_jump = 0.1\n",
    "ri = np.linspace(mu_jump-3.5*sig_jump, mu_jump+3.5*sig_jump, 31)\n",
    "lambdaj = np.linspace(0, 1, 2)\n",
    "\n",
    "# 正态分布权重 N(ri)\n",
    "\n",
    "N_ri = norm.pdf(ri, loc=mu_jump, scale=sig_jump)\n",
    "# print(\"lambdaj[:, None] * ri[None, :]\",(lambdaj[:, None] * ri[None, :]))\n",
    "lambdaj = np.array(lambdaj)  # 应为 [20,]\n",
    "ri = np.array(ri)           # 应为 [50,]\n",
    "N_ri = np.array(N_ri)  \n",
    "\n",
    "# print(ri)\n",
    "print(np.sum(N_ri * 7 * sig_jump/ 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n",
      "lambdaj[:, None] * ri[None, :] (20, 31)\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]\n",
    "#######################################################################################################################\n",
    "# 创建点的范围\n",
    "from scipy.stats import norm\n",
    "sig_jump = 2/48\n",
    "mu_jump = 0.1\n",
    "ri = np.linspace(mu_jump-3*sig_jump, mu_jump+4*sig_jump, 31)\n",
    "lambdaj = np.linspace(0, 1, 20)\n",
    "\n",
    "# 正态分布权重 N(ri)\n",
    "\n",
    "N_ri = norm.pdf(ri, loc=mu_jump, scale=sig_jump)\n",
    "print(\"lambdaj[:, None] * ri[None, :]\",(lambdaj[:, None] * ri[None, :]).shape)\n",
    "lambdaj = np.array(lambdaj)  # 应为 [20,]\n",
    "ri = np.array(ri)           # 应为 [50,]\n",
    "LAMBDA = 30\n",
    "N_ri = np.array(N_ri)  * LAMBDA \n",
    "\n",
    "\n",
    "######## Configuation Parameters #########\n",
    "d      = 1\n",
    "kappa = 1\n",
    "eta = 1\n",
    "\n",
    "\n",
    "D      = 2\n",
    "D_sqrt = onp.sqrt(D)\n",
    "mask   = onp.array([1])\n",
    "dt     = 1e-3\n",
    "tf     = 1\n",
    "n      = 4000\n",
    "n_time_steps = int(tf / dt)\n",
    "store_fac = 5\n",
    "\n",
    "\n",
    "## configure random seed\n",
    "repeatable_seed = False\n",
    "if repeatable_seed:\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    onp.random.seed(42)\n",
    "else:\n",
    "    key = jax.random.PRNGKey(onp.random.randint(10000))\n",
    "\n",
    "\n",
    "## set up forcing parameters\n",
    "drift      = hyf_exapmle1\n",
    "force_args = (kappa,eta,)\n",
    "# drift = active_swimmer\n",
    "# force_args = (gamma,)\n",
    "\n",
    "\n",
    "## initial distribution parameters\n",
    "sig0 = 1.0\n",
    "mu0  = np.zeros(d)\n",
    "\n",
    "### setup optimizer\n",
    "init_learning_rate = 1e-3\n",
    "init_ltol = 1e-6\n",
    "ltol = np.inf\n",
    "gtol = 0.5\n",
    "n_opt_steps = 25\n",
    "n_max_init_opt_steps = int(1e4)\n",
    "\n",
    "\n",
    "### Set up neural network\n",
    "n_hidden = 3\n",
    "n_neurons = 32\n",
    "act = jax.nn.swish\n",
    "residual_blocks = False\n",
    "\n",
    "### output data\n",
    "base_folder   = 'lcy/sbtm-levy/experiments/result'\n",
    "system_folder = 'example1'\n",
    "output_folder = f'{base_folder}/{system_folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 定义你的目录路径\n",
    "directory = 'lcy/sbtm-levy/experiments/result/example1'\n",
    "\n",
    "# 使用 os.makedirs 来确保目录存在，exist_ok=True 表示如果目录已存在则忽略\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 现在你可以安全地保存文件到这个目录\n",
    "# file_path = os.path.join(directory, 'lr=0.01_nf=0.1.npy')\n",
    "# 假设你要保存的 numpy 数组为 data_array\n",
    "# numpy.save(file_path, data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_simulation(\n",
    "    learning_rate: float,\n",
    "    noise_fac: float,\n",
    "    name_str: str\n",
    "):\n",
    "    output_name = f'{name_str}.npy'\n",
    "    sim = DenoisingSequentialSBTM(\n",
    "        n_max_init_opt_steps=n_max_init_opt_steps,\n",
    "        init_learning_rate=init_learning_rate,\n",
    "        init_ltol=init_ltol,\n",
    "        sig0=sig0,\n",
    "        mu0=mu0,\n",
    "        drift=drift,\n",
    "        force_args=force_args,\n",
    "        amp=None,\n",
    "        freq=None,\n",
    "        dt=dt,\n",
    "        D=D,\n",
    "        D_sqrt=D_sqrt,\n",
    "        n=n,\n",
    "        N=1,\n",
    "        d=d,\n",
    "        ltol=ltol,\n",
    "        gtol=gtol,\n",
    "        n_opt_steps=n_opt_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        n_hidden=n_hidden,\n",
    "        n_neurons=n_neurons,\n",
    "        act=act,\n",
    "        residual_blocks=residual_blocks,\n",
    "        interacting_particle_system=False,\n",
    "        key=key,\n",
    "        params_list=[],\n",
    "        all_samples=dict(),\n",
    "        output_folder=output_folder,\n",
    "        output_name=output_name,\n",
    "        n_time_steps=n_time_steps,\n",
    "        noise_fac=noise_fac,\n",
    "        use_ODE=True,\n",
    "        use_SDE=False,\n",
    "        store_fac=store_fac,\n",
    "        save_fac=250,\n",
    "        means={'SDE': [], 'learned': []},\n",
    "        covs={'SDE': [], 'learned': []},\n",
    "        entropies=[],\n",
    "        mask=mask\n",
    "    )\n",
    "\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/astrong/lib/python3.10/site-packages/haiku/_src/initializers.py:127: UserWarning: Explicitly requested dtype float64  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  unscaled = jax.random.truncated_normal(\n",
      "/opt/anaconda3/envs/astrong/lib/python3.10/site-packages/haiku/_src/base.py:658: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  param = init(shape, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2209\n",
      "Number of parameters needed for overparameterization: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial optimization:  10%|█         | 1001/10000 [00:02<00:18, 484.16it/s, loss=9.960092e-07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function DenoisingSequentialSBTM.setup_loss.<locals>.<lambda> at 0x7fb86c35eef0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dynamics: t=0.000:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(all_scores.shape,ri.shape) (20, 31)\n",
      "np.squeeze(np.mean(scores_sumj * ri * N_ri * 6 * sig_jump)) (31,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dynamics: t=0.999: 100%|██████████| 1000/1000 [03:42<00:00,  4.49it/s, grad_norm=0.00018073982, gtol=0.5, loss=-0.10866979, ltol=inf, step_time=0.00821] \n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../')\n",
    "def get_simulation_parameters():\n",
    "    \"\"\"Set up simulation parameters manually for use in a notebook environment.\"\"\"\n",
    "    # 直接指定参数值\n",
    "    learning_rate = 1e-4  # 你可以根据需要修改这个值\n",
    "    noise_fac = 0.01       # 你可以根据需要修改这个值\n",
    "\n",
    "    # 创建一个描述字符串\n",
    "    name_str = f'lr={learning_rate}_nf={noise_fac}_tf={tf}'\n",
    "    \n",
    "    # 返回这些参数\n",
    "    return learning_rate, noise_fac, name_str\n",
    "\n",
    "\n",
    "sim = construct_simulation(*get_simulation_parameters())\n",
    "sim.initialize_network_and_optimizer()\n",
    "sim.fit_init(cpu=cpu, gpu=gpu)\n",
    "sim.initialize_forcing()\n",
    "sim.solve_fpe_sequential(cpu=cpu, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
