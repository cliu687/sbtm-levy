{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  \n",
    "import jax\n",
    "from jax import numpy as np\n",
    "from jax import vmap\n",
    "import optax\n",
    "import numpy as onp\n",
    "from typing import Callable, Tuple\n",
    "import sys\n",
    "import argparse\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.85'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from typing import Optional, Tuple, Callable, Union\n",
    "import jax\n",
    "from jax import vmap\n",
    "from dataclasses import dataclass\n",
    "\n",
    "###################################################################################################################\n",
    "def construct_mlp_layers(\n",
    "    n_hidden: int,\n",
    "    n_neurons: int, \n",
    "    act: Callable[[np.ndarray], np.ndarray], \n",
    "    output_dim: int,\n",
    "    residual_blocks: bool = True \n",
    ") -> list:\n",
    "    \"\"\"Make a list containing the layers of an MLP.\n",
    "\n",
    "    Args:\n",
    "        n_hidden: Number of hidden layers in the MLP.\n",
    "        n_neurons: Number of neurons per hidden layer.\n",
    "        act: Activation function.\n",
    "        output_dim: Dimension of the output.\n",
    "        residual_blocks: Whether or not to use residual blocks.\n",
    "    \"\"\"\n",
    "    layers = [] \n",
    "\n",
    "    resid_act = lambda x: x + act(x)\n",
    "    for layer in range(n_hidden):\n",
    "        ## construct layer\n",
    "        if layer == 0 or not residual_blocks:\n",
    "            layers = layers + [\n",
    "                    hk.Linear(n_neurons),\n",
    "                    act\n",
    "                ]\n",
    "        else:\n",
    "            layers = layers + [\n",
    "                    hk.Linear(n_neurons),\n",
    "                    resid_act\n",
    "                ]\n",
    "\n",
    "\n",
    "    ## construct output layer\n",
    "    layers = layers + [hk.Linear(output_dim)] \n",
    "    return layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "def construct_score_network(\n",
    "    d: int,\n",
    "    n_hidden: int,\n",
    "    n_neurons: int,\n",
    "    act: Callable[[np.ndarray], np.ndarray],\n",
    "    residual_blocks: bool = True,\n",
    "    is_gradient: bool = False\n",
    ") -> Tuple[Callable, Callable]:\n",
    "    \"\"\"Construct a score network for a simpler system\n",
    "    that does not consist of interacting particles.\n",
    "\n",
    "    Args:\n",
    "        d: System dimension.\n",
    "        n_hidden: Number of hidden layers in the network.\n",
    "        n_neurons: Number of neurons per layer.\n",
    "        act: Activation function.\n",
    "        residual_blocks: Whether or not to use residual blocks.\n",
    "        is_gradient: Whether or not to compute the score as the gradient of a potential.\n",
    "    \"\"\"\n",
    "    output_dim = 1 if is_gradient else d \n",
    "    net = lambda x: hk.Sequential(\n",
    "        construct_mlp_layers(n_hidden, n_neurons, act, output_dim, residual_blocks)\n",
    "    )(x)\n",
    "\n",
    "    if is_gradient:\n",
    "        potential = lambda x: np.squeeze(net(x)) \n",
    "        score = jax.grad(potential)\n",
    "        return hk.without_apply_rng(hk.transform(score)), \\\n",
    "                hk.without_apply_rng(hk.transform(potential))\n",
    "    else:\n",
    "        return hk.without_apply_rng(hk.transform(net)), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "import jax.numpy as np\n",
    "from typing import Tuple, Callable, Union, Optional\n",
    "from jax import jit, vmap, value_and_grad\n",
    "from jaxopt.linear_solve import solve_cg\n",
    "from functools import partial\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "def grad_log_rho0(\n",
    "    sample: np.ndarray,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial potential. Assumed to be an isotropic Gaussian.\"\"\"\n",
    "    return -(sample - mu0) / sig0**2\n",
    "\n",
    "def rho0(\n",
    "    sample: np.ndarray,\n",
    "    sig0: float = 1,\n",
    "    mu0: np.ndarray = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the \n",
    "    ial potential. Assumed to be an isotropic Gaussian.\"\"\"\n",
    "    sig0 = 1\n",
    "    mu0 = 0\n",
    "    return np.exp(-(sample - mu0)**2 / (2 * sig0**2))\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(4, 5, 7))\n",
    "def B_init_loss(\n",
    "    B_params: np.ndarray,\n",
    "    samples: np.ndarray,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray,\n",
    "    B_apply_score: Callable[[hk.Params, np.ndarray, Optional[float]], np.ndarray],\n",
    "    time_dependent: bool = False, \n",
    "    frame_end: float = 0,\n",
    "    nt: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial loss, assuming access to \\nabla \\log \\rho0.\"\"\"\n",
    "\n",
    "    grad_log_rho_evals = vmap(\n",
    "        lambda sample:  D * grad_log_rho0(sample, sig0, mu0)\n",
    "    )(samples) \n",
    "    \n",
    "    score_evals = vmap(\n",
    "        lambda sample:   B_apply_score(B_params, sample)\n",
    "    )(samples)\n",
    "\n",
    "    score = np.sum(   ( score_evals - (grad_log_rho_evals) )**2) \\\n",
    "            / np.sum((grad_log_rho_evals)**2)\n",
    "\n",
    "    return score\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(4, 5, 7))\n",
    "def L_init_loss(\n",
    "    L_params: np.ndarray,\n",
    "    samples: np.ndarray,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray,\n",
    "    L_apply_score: Callable[[hk.Params, np.ndarray, Optional[float]], np.ndarray],\n",
    "    time_dependent: bool = False, \n",
    "    frame_end: float = 0,\n",
    "    nt: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the initial loss, assuming access to \\nabla \\log \\rho0.\"\"\"\n",
    "    \n",
    "    score_evals = vmap(\n",
    "        lambda sample:   L_apply_score(L_params, sample)\n",
    "    )(samples)\n",
    "\n",
    "    @jax.jit\n",
    "    def levypart(sample):\n",
    "        sample = np.array(sample)   \n",
    "        lambdaj_ri_matrix = lambdaj[:, None] * ri[None, :] \n",
    "        lambdaj_ri_matrix = np.expand_dims(lambdaj_ri_matrix, axis=2)\n",
    "        expanded_samples = sample - lambdaj_ri_matrix \n",
    "        all_scores = (jax.vmap(jax.vmap(rho0))(expanded_samples)).squeeze()\n",
    "        scores_sumj = np.mean(all_scores, axis=0)\n",
    "        return np.mean(scores_sumj * ri * N_ri * 7 * sig_jump) / rho0(sample)\n",
    "\n",
    "    LEVYpart = vmap(levypart, in_axes=0)(samples)\n",
    "    score = np.sum(   ( score_evals - ( - LEVYpart) )**2) \\\n",
    "            / np.sum((-LEVYpart)**2)\n",
    "\n",
    "    return score\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@jit\n",
    "def compute_grad_norm(\n",
    "    grads: hk.Params\n",
    ") -> float:\n",
    "    \"\"\" Computes the norm of the gradient, where the gradient is input\n",
    "    as an hk.Params object (treated as a PyTree).\"\"\"\n",
    "    flat_params = ravel_pytree(grads)[0]\n",
    "    return np.linalg.norm(flat_params) / np.sqrt(flat_params.size)\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(2, 3))\n",
    "def update(\n",
    "    params: hk.Params, \n",
    "    opt_state: optax.OptState, \n",
    "    opt: optax.GradientTransformation,\n",
    "    loss_func: Callable[[hk.Params], float],\n",
    "    loss_func_args: Tuple = tuple(), \n",
    ") -> Tuple[hk.Params, optax.OptState, float, hk.Params]:\n",
    "    \"\"\"Update the neural network.\n",
    "\n",
    "    Args:\n",
    "        params: Parameters to optimize over.\n",
    "        opt_state: State of the optimizer.\n",
    "        opt: Optimizer itself.\n",
    "        loss_func: Loss function for the parameters.\n",
    "    \"\"\"\n",
    "    loss_value, grads = value_and_grad(loss_func)(params, *loss_func_args)\n",
    "    \n",
    "    updates, opt_state = opt.update(grads, opt_state, params=params) \n",
    "    new_params = optax.apply_updates(params, updates) \n",
    "    return new_params, opt_state, loss_value, grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, vmap\n",
    "import jax\n",
    "from functools import partial\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import haiku as hk\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(6,7,8)) \n",
    "def update_particles(\n",
    "    particle_pos: np.ndarray,\n",
    "    t: float,\n",
    "    B_params: hk.Params,\n",
    "    L_params: hk.Params,\n",
    "    D: Union[np.ndarray, float], \n",
    "    dt: float,\n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray], \n",
    "    B_apply_score: Callable[[hk.Params, np.ndarray], np.ndarray], \n",
    "    L_apply_score: Callable[[hk.Params, np.ndarray], np.ndarray],\n",
    "    mask: np.ndarray = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Take a forward Euler step and update the particles.\"\"\"\n",
    "    if mask is not None:\n",
    "        score_term = -mask * B_apply_score(B_params, particle_pos) - mask * L_apply_score(L_params, particle_pos)\n",
    "    else:\n",
    "        score_term = -B_apply_score(B_params, particle_pos) - L_apply_score(L_params, particle_pos)\n",
    "\n",
    "    return particle_pos + dt*(forcing(particle_pos, t) + score_term)\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "@partial(jit, static_argnums=(5, 6))\n",
    "def update_particles_EM(\n",
    "    particle_pos: np.ndarray,\n",
    "    t: float,\n",
    "    D_sqrt: Union[np.ndarray, float], \n",
    "    dt: float,\n",
    "    key: np.ndarray, \n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray],\n",
    "    noisy: bool = True,\n",
    "    mask: np.ndarray = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Take a step forward via Euler-Maruyama to update the particles.\"\"\"\n",
    "\n",
    "    if noisy:\n",
    "        noise = np.sqrt(2*dt) * jax.random.normal(key, shape=particle_pos.shape)\n",
    "        if mask is not None:\n",
    "            brownian = -D_sqrt * mask * noise\n",
    "        else:\n",
    "            brownian = -D_sqrt * noise\n",
    "\n",
    "        return particle_pos + dt*forcing(particle_pos, t) + brownian\n",
    "    else:\n",
    "        return particle_pos + dt*forcing(particle_pos, t)\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "def rollout_EM_trajs(\n",
    "    x0s: np.ndarray,\n",
    "    nsteps: int,\n",
    "    t0: float,\n",
    "    dt: float,\n",
    "    key: np.ndarray,\n",
    "    forcing: Callable[[np.ndarray, float], np.ndarray],\n",
    "    D_sqrt: Union[np.ndarray, float],\n",
    "    noisy: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Given a set of initial conditions, create a stochastic trajectory \n",
    "    via Euler-Maruyama. Useful for constructing a baseline against which to compare\n",
    "    the moments.\n",
    "\n",
    "    Args:\n",
    "    ------\n",
    "    x0s: Initial condition. Dimension = n x d where n is the number of samples \n",
    "         and d is the dimension of the system.\n",
    "    nsteps: Number of steps of Euler-Maruyama to take.\n",
    "    t0: initial time.\n",
    "    dt: Timestep.\n",
    "    key: jax PRNG key.\n",
    "    forcing: Forcing to apply to the particles.\n",
    "    D_sqrt: Square root of the diffusion matrix.\n",
    "    \"\"\"\n",
    "    n, d = x0s.shape\n",
    "    trajs = onp.zeros((nsteps+1, n, d)) \n",
    "    trajs[0] = x0s\n",
    "    step_sample = \\\n",
    "            lambda sample, t, key: update_particles_EM(sample, t, D_sqrt, \n",
    "                                                       dt, key, forcing, noisy)\n",
    "    step_samples = vmap(step_sample, in_axes=(0, None, 0), out_axes=0)\n",
    "\n",
    "    \n",
    "    for curr_step in tqdm(range(nsteps)):\n",
    "        t = t0 + curr_step*dt\n",
    "        keys = jax.random.split(key, num=n) \n",
    "        trajs[curr_step+1] = step_samples(trajs[curr_step], t, keys)\n",
    "        key = keys[-1] \n",
    "\n",
    "    return trajs, key \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, vmap\n",
    "from jax.tree_util import tree_map\n",
    "import jax\n",
    "from functools import partial\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import haiku as hk\n",
    "from typing import Callable, Tuple, Union\n",
    "import optax\n",
    "import dill as pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from jaxlib.xla_extension import Device\n",
    "\n",
    "Time = float\n",
    "\n",
    "\n",
    "def fit_initial_condition(\n",
    "    n_max_opt_steps: int, \n",
    "    ltol: float, \n",
    "    B_params: hk.Params, \n",
    "    L_params: hk.Params,\n",
    "    sig0: float,\n",
    "    mu0: np.ndarray,\n",
    "    B_score_network: Callable[[hk.Params, np.ndarray], np.ndarray],\n",
    "    L_score_network: Callable[[hk.Params, np.ndarray], np.ndarray],\n",
    "    B_opt: optax.GradientTransformation,\n",
    "    B_opt_state: optax.OptState,\n",
    "    L_opt: optax.GradientTransformation,\n",
    "    L_opt_state: optax.OptState,\n",
    "    samples: np.ndarray,\n",
    "    time_dependent: bool = False,\n",
    "    frame_end: float = 0,\n",
    "    nt: int = 0\n",
    ") -> hk.Params:\n",
    "    \"\"\"Fit the score for the initial condition.\n",
    "\n",
    "    Args:\n",
    "        n_opt_steps: Number of optimization steps before the norm of the gradient \n",
    "                     is checked.\n",
    "        gtol: Tolerance on the norm of the gradient.\n",
    "        ltol: Tolerance on the relative error.\n",
    "        params: Parameters to optimize over.\n",
    "        sig0: Standard deviation of the target initial condition.\n",
    "        mu0: Mean of the target initial condition.\n",
    "        score_network: Function mapping parameters and a sample to the network output.\n",
    "        opt: Optimizer.\n",
    "        opt_state: State of the optimizer.\n",
    "        samples: Samples to optimizer over.\n",
    "    \"\"\"\n",
    "    \n",
    "    B_apply_score = B_score_network.apply \n",
    "    B_loss_func = lambda B_params: \\\n",
    "            B_init_loss(B_params, samples, sig0, mu0, B_apply_score, \n",
    "                             time_dependent, frame_end, nt)\n",
    "\n",
    "    L_apply_score = L_score_network.apply \n",
    "    L_loss_func = lambda L_params: \\\n",
    "            L_init_loss(L_params, samples, sig0, mu0, L_apply_score, \n",
    "                             time_dependent, frame_end, nt)\n",
    "            \n",
    "    B_loss_val = np.inf\n",
    "    L_loss_val = np.inf\n",
    "    try:\n",
    "        with tqdm(range(n_max_opt_steps)) as pbar:\n",
    "            pbar.set_description(\"Initial optimization\")\n",
    "            for curr_step in pbar:\n",
    "                try:\n",
    "                    B_params, B_opt_state, B_loss_val, B_grads = update(B_params, B_opt_state, B_opt, B_loss_func)\n",
    "                    L_params, L_opt_state, L_loss_val, L_grads = update(L_params, L_opt_state, L_opt, L_loss_func)\n",
    "                    pbar.set_postfix(B_loss=B_loss_val, L_loss=L_loss_val)\n",
    "                    if (B_loss_val < ltol) and (L_loss_val < ltol):\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error at step {curr_step}: {e}\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing the progress bar: {e}\")\n",
    "\n",
    "    return B_params, L_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drifts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from jax.lax import stop_gradient\n",
    "import jax.numpy as np\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "def example_1(\n",
    "    x: np.ndarray,\n",
    "    t: float,\n",
    "    # gamma: float\n",
    "    V0: float,\n",
    "    Gamma: float,\n",
    "    L: float,\n",
    ") -> np.ndarray:\n",
    "    del t\n",
    "    return -(V0/Gamma) * (2*np.pi/L * np.cos(2*np.pi*x/L ) + 1*np.pi/L * np.cos(4*np.pi*x/L ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBTM-SIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple, Union\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as onp\n",
    "from jaxlib.xla_extension import Device\n",
    "import optax\n",
    "\n",
    "\n",
    "State = onp.ndarray\n",
    "Time = float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SBTMSim:\n",
    "    \"\"\"\n",
    "    Base class for all SBTM simulations.\n",
    "    Contains simulation parameters common to all SBTM approaches.\n",
    "    \"\"\"\n",
    "    # initial condition fitting\n",
    "    n_max_init_opt_steps: int\n",
    "    init_learning_rate: float\n",
    "    init_ltol: float\n",
    "    sig0: float\n",
    "    mu0: onp.ndarray\n",
    "\n",
    "    # system parameters\n",
    "    drift: Callable[[State, Time], State]\n",
    "    force_args: Tuple\n",
    "    amp: Callable[[Time], float]\n",
    "    freq: float\n",
    "    dt: float\n",
    "    D: onp.ndarray\n",
    "    D_sqrt: onp.ndarray\n",
    "    n: int\n",
    "    d: int\n",
    "    N: int\n",
    "\n",
    "    # timestepping\n",
    "    ltol: float\n",
    "    gtol: float\n",
    "    n_opt_steps: int\n",
    "    learning_rate: float\n",
    "\n",
    "    # network parameters\n",
    "    n_hidden: int\n",
    "    n_neurons: int\n",
    "    act: Callable[[State], State]\n",
    "    residual_blocks: bool\n",
    "    interacting_particle_system: bool\n",
    "\n",
    "    # general simulation parameters\n",
    "    key: onp.ndarray\n",
    "    B_params_list: list\n",
    "    L_params_list: list\n",
    "    all_samples: dict\n",
    "\n",
    "    # output information\n",
    "    output_folder: str\n",
    "    output_name: str\n",
    "\n",
    "\n",
    "    def __init__(self, data_dict: dict) -> None:\n",
    "        self.__dict__ = data_dict.copy()\n",
    "\n",
    "    def initialize_forcing(self) -> None:\n",
    "        self.forcing = lambda x, t: self.drift(x, t, *self.force_args)\n",
    "\n",
    "    def initialize_network_and_optimizer(self) -> None:\n",
    "        \"\"\"Initialize the network parameters and optimizer.\"\"\"\n",
    "\n",
    "        self.B_score_network, self.potential_network= \\\n",
    "            construct_score_network(\n",
    "                self.d,\n",
    "                self.n_hidden,\n",
    "                self.n_neurons,\n",
    "                self.act,\n",
    "                is_gradient=False\n",
    "            )\n",
    "\n",
    "        self.L_score_network, self.potential_network= \\\n",
    "            construct_score_network(\n",
    "                self.d,\n",
    "                self.n_hidden,\n",
    "                self.n_neurons,\n",
    "                self.act,\n",
    "                is_gradient=False\n",
    "            )\n",
    "      \n",
    "        example_x = onp.zeros(self.d) \n",
    "        self.key, sk = jax.random.split(self.key) \n",
    "        \n",
    "        B_init_params = self.B_score_network.init(self.key, example_x) \n",
    "        self.B_params_list = [B_init_params]   \n",
    "        network_size = jax.flatten_util.ravel_pytree(B_init_params)[0].size \n",
    "        print(f'Number of parameters (B): {network_size}')\n",
    "        print(f'Number of parameters needed for overparameterization (B): ' \\\n",
    "                + f'{self.n*example_x.size}')\n",
    "        # set up the optimizer\n",
    "        self.B_opt = optax.radam(self.learning_rate) \n",
    "        self.B_opt_state = self.B_opt.init(B_init_params) \n",
    "        # set up batching for the score\n",
    "        self.B_batch_score = jax.vmap(self.B_score_network.apply, in_axes=(None, 0))\n",
    "        \n",
    "        L_init_params = self.L_score_network.init(self.key, example_x) \n",
    "        self.L_params_list = [L_init_params]   \n",
    "        network_size = jax.flatten_util.ravel_pytree(L_init_params)[0].size \n",
    "        print(f'Number of parameters (L): {network_size}')\n",
    "        print(f'Number of parameters needed for overparameterization (L): ' \\\n",
    "                + f'{self.n*example_x.size}')\n",
    "        # set up the optimizer\n",
    "        self.L_opt = optax.radam(self.learning_rate) \n",
    "        self.L_opt_state = self.L_opt.init(L_init_params) \n",
    "        # set up batching for the score\n",
    "        self.L_batch_score = jax.vmap(self.L_score_network.apply, in_axes=(None, 0))\n",
    "\n",
    "    def fit_init(self, cpu: Device, gpu: Device) -> None:\n",
    "        \"\"\"Fit the initial condition.\"\"\"\n",
    "        # draw samples\n",
    "        samples_shape = (self.n, self.N*self.d) \n",
    "        init_samples = self.sig0*onp.random.randn(*samples_shape) + self.mu0[None, :] \n",
    "        \n",
    "        # set up optimizer\n",
    "        B_init_params = jax.device_put(self.B_params_list[0], gpu) \n",
    "        B_opt = optax.adabelief(self.init_learning_rate) \n",
    "        B_opt_state = B_opt.init(B_init_params)\n",
    "\n",
    "        L_init_params = jax.device_put(self.L_params_list[0], gpu) \n",
    "        L_opt = optax.adabelief(self.init_learning_rate) \n",
    "        L_opt_state = L_opt.init(L_init_params)\n",
    "        \n",
    "        B_init_params, L_init_params = fit_initial_condition(\n",
    "                            self.n_max_init_opt_steps,\n",
    "                            self.init_ltol,\n",
    "                            B_init_params,\n",
    "                            L_init_params,\n",
    "                            self.sig0,\n",
    "                            self.mu0,\n",
    "                            self.B_score_network,\n",
    "                            self.L_score_network,\n",
    "                            B_opt,\n",
    "                            B_opt_state,\n",
    "                            L_opt,\n",
    "                            L_opt_state,\n",
    "                            init_samples\n",
    "                        )\n",
    "        self.B_params_list = [jax.device_put(B_init_params, device=cpu)] \n",
    "        self.L_params_list = [jax.device_put(L_init_params, device=cpu)] \n",
    "        self.all_samples = {'SDE': [init_samples], 'learned': [init_samples]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Entropy Calculation #######\n",
    "@partial(jit, static_argnums=(5, 6, 7, 8, 9))\n",
    "def compute_sample_entropy_rate(\n",
    "    sample: np.ndarray,\n",
    "    t: float,\n",
    "    B_params: hk.Params,\n",
    "    L_params: hk.Params,\n",
    "    D: Union[np.ndarray, float],\n",
    "    forcing: Callable[[State, Time], State],\n",
    "    B_score_network: Callable[[hk.Params, State], State],\n",
    "    L_score_network: Callable[[hk.Params, State], State],\n",
    "    noise_free: bool,\n",
    "    div: bool,\n",
    ") -> float:\n",
    "    B_st = B_score_network.apply(B_params, sample)\n",
    "    L_st = B_score_network.apply(L_params, sample)\n",
    "    vt = forcing(sample, t) - B_st - L_st\n",
    "    return np.sum(vt*vt), np.sum(forcing(sample, t) * vt), np.sum(- L_st * vt), np.sum(- B_st * vt)\n",
    "    \n",
    "@partial(jit, static_argnums=( 5, 6, 7, 8, 9))\n",
    "def compute_entropy_rate(\n",
    "    samples: np.ndarray, \n",
    "    t: float, \n",
    "    B_params: hk.Params, \n",
    "    L_params: hk.Params, \n",
    "    D: np.ndarray,\n",
    "    forcing: Callable[[State, Time], State],\n",
    "    B_score_network: Callable,\n",
    "    L_score_network: Callable,\n",
    "    noise_free: bool,\n",
    "    div: bool\n",
    ") -> float:\n",
    "    Nones = (None,)*9\n",
    "    results = vmap(\n",
    "        compute_sample_entropy_rate, in_axes=(0, *Nones)\n",
    "    )(samples, t, B_params, L_params, D, forcing, B_score_network, L_score_network, noise_free, div)\n",
    "\n",
    "    eprtot = np.mean(results[0])\n",
    "    eprm = np.mean(results[1])\n",
    "    epract = np.mean(results[2])\n",
    "    eprsys = np.mean(results[3])\n",
    "    return eprtot, eprm, epract, eprsys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBTM-SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "import numpy as onp\n",
    "import dill as pickle\n",
    "import time\n",
    "from jaxlib.xla_extension import Device\n",
    "from haiku import Params\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SequentialSBTM(SBTMSim): \n",
    "    n_time_steps: int\n",
    "    use_SDE: bool\n",
    "    use_ODE: bool\n",
    "    save_fac: int \n",
    "    store_fac: int \n",
    "    means: dict\n",
    "    covs: dict \n",
    "    epr_tot: list \n",
    "    epr_m: list \n",
    "    epr_act: list \n",
    "    epr_sys: list \n",
    "    mask: np.ndarray\n",
    "\n",
    "\n",
    "    def setup_loss(self):\n",
    "        \"\"\"Define the loss function. \"\"\"\n",
    "        raise NotImplementedError(\"Please implement in the inheriting class.\") \n",
    "\n",
    "\n",
    "    \n",
    "    def setup_loss_fn_args(self, gpu: Device) -> Tuple:\n",
    "        \"\"\"Define the arguments to the loss function other than parameters.\"\"\"\n",
    "        raise NotImplementedError(\"Please implement in the inheriting class.\")\n",
    "        \n",
    "\n",
    "    def setup_batched_steppers(self):\n",
    "        \"\"\"Construct convenience functions to step the particles.\"\"\"\n",
    "        self.step_learned = vmap(\n",
    "                lambda B_params, L_params, t, sample: update_particles(\n",
    "                    sample, \n",
    "                    t, \n",
    "                    B_params, \n",
    "                    L_params,\n",
    "                    self.D, \n",
    "                    self.dt,  \n",
    "                    self.forcing, \n",
    "                    self.B_score_network.apply,\n",
    "                    self.L_score_network.apply,\n",
    "                    self.mask\n",
    "                ),\n",
    "                in_axes=(None, None, None, 0),\n",
    "                out_axes=0\n",
    "        )\n",
    "\n",
    "        self.step_SDE = vmap(\n",
    "                lambda t, sample, key: update_particles_EM(\n",
    "                    sample, \n",
    "                    t, \n",
    "                    self.D_sqrt, \n",
    "                    self.dt, \n",
    "                    key,  \n",
    "                    self.forcing,\n",
    "                    True,\n",
    "                    self.mask\n",
    "                ),\n",
    "                in_axes=(None, 0, 0),\n",
    "                out_axes=0\n",
    "        )\n",
    "\n",
    "    def step_samples(\n",
    "        self,\n",
    "        step,\n",
    "        B_params: Params,\n",
    "        L_params: Params,\n",
    "        t: float,\n",
    "        samples: np.ndarray,\n",
    "        SDE_samples: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Step and save both the SDE and ODE samples.\"\"\"\n",
    "        # step learned and SDE particles\n",
    "        samples = self.step_learned(B_params, L_params, t, samples)\n",
    "        keys = jax.random.split(self.key, num=self.n)\n",
    "        SDE_samples = self.step_SDE(t, SDE_samples, keys)\n",
    "        self.key = keys[-1]\n",
    "\n",
    "        # save new samples\n",
    "        if (step+1) % self.store_fac == 0:\n",
    "            self.all_samples['learned'].append(onp.array(samples))\n",
    "            self.all_samples['SDE'].append(onp.array(SDE_samples))\n",
    "\n",
    "        return samples, SDE_samples\n",
    "\n",
    "\n",
    "    def setup_learning_samples(\n",
    "        self,\n",
    "        samples: np.ndarray,\n",
    "        SDE_samples: np.ndarray\n",
    "    ) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Set up samples to optimize over.\"\"\"\n",
    "        if self.use_ODE and self.use_SDE:\n",
    "            opt_samples = (np.vstack((samples, SDE_samples)),)\n",
    "        elif self.use_ODE:\n",
    "            opt_samples = (samples,)\n",
    "        elif self.use_SDE:\n",
    "            opt_samples = (SDE_samples,)\n",
    "        else:\n",
    "            raise ValueError('Need to specify learning from ODE or SDE.')\n",
    "\n",
    "        return opt_samples\n",
    "    \n",
    "    def compute_epr(\n",
    "        self,\n",
    "        B_params: Params,\n",
    "        L_params: Params,\n",
    "        t: float,\n",
    "        samples: np.ndarray, \n",
    "        SDE_samples: np.ndarray\n",
    "    ) -> None:\n",
    "        ## entropy\n",
    "        eprtot, eprm, epract, eprsys = compute_entropy_rate(\n",
    "            samples, t, B_params, L_params, self.D, self.forcing, \n",
    "            self.B_score_network, self.L_score_network, noise_free=False, div=True\n",
    "        )\n",
    "        self.epr_tot.append(eprtot)\n",
    "        self.epr_m.append(eprm)\n",
    "        self.epr_act.append(epract)\n",
    "        self.epr_sys.append(eprsys)\n",
    "\n",
    "\n",
    "    def solve_fpe_sequential(self, cpu: Device, gpu: Device):\n",
    "        self.setup_loss()\n",
    "        self.setup_batched_steppers()\n",
    "        nt = len(self.B_params_list) - 1\n",
    "\n",
    "        \n",
    "        B_params = jax.device_put(self.B_params_list[-1], gpu) \n",
    "        B_opt_state = jax.device_put(self.B_opt_state, gpu)\n",
    "        L_params = jax.device_put(self.L_params_list[-1], gpu) \n",
    "        L_opt_state = jax.device_put(self.L_opt_state, gpu)\n",
    "        samples = jax.device_put(self.all_samples['learned'][-1], gpu) \n",
    "        SDE_samples = jax.device_put(self.all_samples['SDE'][-1], gpu) \n",
    "\n",
    "        self.compute_epr(B_params, L_params, t=0, samples=samples, SDE_samples=SDE_samples)\n",
    "        \n",
    "        with tqdm(range(self.n_time_steps)) as pbar: \n",
    "            for step in pbar:\n",
    "                t = (nt*self.store_fac + step)*self.dt\n",
    "                pbar.set_description(f\"Dynamics: t={t:.3f}\") \n",
    "\n",
    "                samples, SDE_samples = self.step_samples(step, B_params, L_params, t, samples, SDE_samples) \n",
    "                opt_samples = self.setup_learning_samples(samples, SDE_samples)\n",
    "\n",
    "                ## perform the optimization\n",
    "                B_loss_value, B_grad_norm = np.inf, np.inf\n",
    "                L_loss_value, L_grad_norm = np.inf, np.inf\n",
    "                num_steps_taken = 0\n",
    "                while (B_grad_norm > self.gtol) or (L_grad_norm > self.gtol):\n",
    "                    for curr_opt_step in range(self.n_opt_steps): \n",
    "                        loss_func_args = opt_samples + self.setup_loss_fn_args(gpu)\n",
    "                        start_time = time.time()\n",
    "                        B_params, B_opt_state, B_loss_value, B_grads \\\n",
    "                                = update(\n",
    "                                        B_params, \n",
    "                                        B_opt_state, \n",
    "                                        self.B_opt, \n",
    "                                        self.B_loss_func, \n",
    "                                        loss_func_args\n",
    "                                    )\n",
    "                        L_params, L_opt_state, L_loss_value, L_grads \\\n",
    "                                = update(\n",
    "                                        L_params, \n",
    "                                        L_opt_state, \n",
    "                                        self.L_opt, \n",
    "                                        self.L_loss_func, \n",
    "                                        loss_func_args\n",
    "                                    )\n",
    "                        end_time = time.time()\n",
    "\n",
    "                    B_grad_norm = compute_grad_norm(B_grads)\n",
    "                    L_grad_norm = compute_grad_norm(L_grads)\n",
    "                    pbar.set_postfix(\n",
    "                        B_loss=B_loss_value, L_loss=L_loss_value, ltol=self.ltol,\n",
    "                        B_grad_norm=B_grad_norm, L_grad_norm=L_grad_norm, gtol=self.gtol,\n",
    "                        step_time=end_time-start_time\n",
    "                    ) \n",
    "                    \n",
    "                self.compute_epr(B_params, L_params, t, samples, SDE_samples)\n",
    "                \n",
    "                if (step+1) % self.store_fac == 0:\n",
    "                    self.B_params_list.append(jax.device_put(B_params, cpu))\n",
    "                    self.L_params_list.append(jax.device_put(L_params, cpu))\n",
    "                    \n",
    "\n",
    "                if (step+1) % self.save_fac == 0:\n",
    "                    self.save_data()\n",
    "        self.save_data()\n",
    "\n",
    "\n",
    "    def save_data(self):\n",
    "        data = vars(self).copy()\n",
    "        pickle.dump(data, open(f'{self.output_folder}/{self.output_name}', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenoisingSequentialSBTM(SequentialSBTM):\n",
    "    noise_fac: float \n",
    "\n",
    "    def setup_loss(self):\n",
    "        @jax.jit\n",
    "        def score_for_sample(params, sample):\n",
    "            score = self.mask*self.L_score_network.apply(params, sample)\n",
    "            \n",
    "            sample = np.array(sample)   \n",
    "\n",
    "            lambdaj_ri_matrix = lambdaj[:, None] * ri[None, :]  \n",
    "            lambdaj_ri_matrix = np.expand_dims(lambdaj_ri_matrix, axis=2)\n",
    "            expanded_samples = sample + lambdaj_ri_matrix  \n",
    "\n",
    "            def apply_network(exp_sample):\n",
    "                return self.L_score_network.apply(params, exp_sample)\n",
    "\n",
    "            all_scores = (jax.vmap(jax.vmap(apply_network))(expanded_samples)).squeeze()\n",
    "            \n",
    "            scores_sumj = np.mean(all_scores, axis=0)\n",
    "            \n",
    "            \n",
    "            return np.sum(self.noise_fac*score**2) + 2 * self.noise_fac* np.squeeze(np.mean(scores_sumj * ri * N_ri * 7 * sig_jump))\n",
    "            \n",
    "        def sample_denoising_loss(\n",
    "            params: Params,\n",
    "            sample: np.ndarray,\n",
    "            noise: np.ndarray,\n",
    "        ) -> float: \n",
    "            \"\"\"\n",
    "            Compute the denoising loss on a single sample, using antithetic sampling\n",
    "            over the noise for variance reduction.\n",
    "            \"\"\"\n",
    "            loss = 0\n",
    "            for sign in [-1, 1]:\n",
    "                perturbed_sample = sample + self.noise_fac*sign*noise\n",
    "                score = self.mask*self.B_score_network.apply(params, perturbed_sample)\n",
    "                loss += np.sum(self.noise_fac*score**2 + (2*D) *sign*score*noise)\n",
    "\n",
    "            return np.squeeze(loss / 2) \n",
    "\n",
    "        self.B_loss_func = lambda B_params, samples, noise: np.mean(vmap(sample_denoising_loss, in_axes=(None, 0, 0))(B_params, samples, noise))  \n",
    "        self.L_loss_func = lambda L_params, samples, noise: np.mean(vmap(score_for_sample, in_axes=(None, 0))(L_params, samples))\n",
    "\n",
    "        \n",
    "        if self.use_SDE and self.use_ODE:\n",
    "            self.n_train_samples = 2*self.n\n",
    "        else:\n",
    "            self.n_train_samples = self.n\n",
    "\n",
    "\n",
    "    def setup_loss_fn_args(self, gpu: Device) -> Tuple:\n",
    "        \"\"\"Set up noise arguments for the loss function. \"\"\"\n",
    "        noises = onp.random.randn(self.n_train_samples, self.d*self.N)\n",
    "        loss_func_args = (jax.device_put(noises, gpu),)\n",
    "        return loss_func_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "from scipy.stats import norm\n",
    "sig_jump = 2/48\n",
    "mu_jump = 0.1\n",
    "ri = np.linspace(mu_jump-3*sig_jump, mu_jump+4*sig_jump, 31)\n",
    "lambdaj = np.linspace(0, 1, 20)\n",
    "N_ri = norm.pdf(ri, loc=mu_jump, scale=sig_jump)\n",
    "lambdaj = np.array(lambdaj)  \n",
    "ri = np.array(ri)           \n",
    "LAMBDA = 30\n",
    "N_ri = np.array(N_ri)  * LAMBDA \n",
    "\n",
    "\n",
    "######## Configuation Parameters #########\n",
    "d      = 1\n",
    "kappa = 1\n",
    "eta = 1\n",
    "\n",
    "\n",
    "D      = 2\n",
    "\n",
    "kbT = 4.114\n",
    "V0 = 5*4.114\n",
    "Gamma = 3.25\n",
    "L = 40\n",
    "\n",
    "D      = kbT / Gamma\n",
    "\n",
    "D_sqrt = onp.sqrt(D)\n",
    "mask   = onp.array([1])\n",
    "dt     = 1e-3\n",
    "tf     = 100\n",
    "n      = 4000\n",
    "n_time_steps = int(tf / dt)\n",
    "store_fac = 5\n",
    "\n",
    "\n",
    "## configure random seed\n",
    "repeatable_seed = False\n",
    "if repeatable_seed:\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    onp.random.seed(42)\n",
    "else:\n",
    "    key = jax.random.PRNGKey(onp.random.randint(10000))\n",
    "\n",
    "\n",
    "## set up forcing parameters\n",
    "drift      = example_1\n",
    "force_args = (kappa,eta,)\n",
    "force_args = (V0,Gamma,L,)\n",
    "\n",
    "\n",
    "## initial distribution parameters\n",
    "sig0 = 1.0\n",
    "mu0  = np.zeros(d)\n",
    "\n",
    "### setup optimizer\n",
    "init_learning_rate = 1e-4\n",
    "init_ltol = 1e-6\n",
    "ltol = np.inf\n",
    "gtol = 0.5\n",
    "n_opt_steps = 25\n",
    "n_max_init_opt_steps = int(1e4)\n",
    "\n",
    "\n",
    "### Set up neural network\n",
    "n_hidden = 3\n",
    "n_neurons = 32\n",
    "act = jax.nn.swish\n",
    "residual_blocks = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_folder = 'lcy/epr-levy/experiments/result'\n",
    "system_folder = 'example1'\n",
    "output_folder = os.path.join(base_folder, system_folder)\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_simulation(\n",
    "    learning_rate: float,\n",
    "    noise_fac: float,\n",
    "    name_str: str\n",
    "):\n",
    "    output_name = f'{name_str}.npy'\n",
    "    sim = DenoisingSequentialSBTM(\n",
    "        n_max_init_opt_steps=n_max_init_opt_steps,\n",
    "        init_learning_rate=init_learning_rate,\n",
    "        init_ltol=init_ltol,\n",
    "        sig0=sig0,\n",
    "        mu0=mu0,\n",
    "        drift=drift,\n",
    "        force_args=force_args,\n",
    "        amp=None,\n",
    "        freq=None,\n",
    "        dt=dt,\n",
    "        D=D,\n",
    "        D_sqrt=D_sqrt,\n",
    "        n=n,\n",
    "        N=1,\n",
    "        d=d,\n",
    "        ltol=ltol,\n",
    "        gtol=gtol,\n",
    "        n_opt_steps=n_opt_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        n_hidden=n_hidden,\n",
    "        n_neurons=n_neurons,\n",
    "        act=act,\n",
    "        residual_blocks=residual_blocks,\n",
    "        interacting_particle_system=False,\n",
    "        key=key,\n",
    "        B_params_list=[],\n",
    "        L_params_list=[],\n",
    "        all_samples=dict(),\n",
    "        output_folder=output_folder,\n",
    "        output_name=output_name,\n",
    "        n_time_steps=n_time_steps,\n",
    "        noise_fac=noise_fac,\n",
    "        use_ODE=True,\n",
    "        use_SDE=False,\n",
    "        store_fac=store_fac,\n",
    "        save_fac=250,\n",
    "        means={'SDE': [], 'learned': []},\n",
    "        covs={'SDE': [], 'learned': []},\n",
    "        epr_tot=[],\n",
    "        epr_m=[],\n",
    "        epr_act=[],\n",
    "        epr_sys=[],\n",
    "        mask=mask\n",
    "    )\n",
    "\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "def get_simulation_parameters():\n",
    "    \"\"\"Set up simulation parameters manually for use in a notebook environment.\"\"\"\n",
    "    learning_rate = 1e-6\n",
    "    noise_fac = 0.01       \n",
    "\n",
    "    name_str = f'lr={learning_rate}_nf={noise_fac}_tf={tf} copy'\n",
    "    return learning_rate, noise_fac, name_str\n",
    "\n",
    "print(jax.devices())\n",
    "gpu = jax.devices('gpu')[0]\n",
    "cpu = jax.devices('cpu')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
